{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "text = '''전국 언론 노동자들이 시민사회단체와 함께 네이버의 지역 차별을 규탄하고 개선을 촉구하는 무기한 릴레이 시위에 돌입했다.\n",
    "\n",
    "전국언론노조와 전국 민주언론시민연합(민언련)은 1일 오전 11시 30분 경기도 성남시의 네이버 본사(그린팩토리) 앞에서 1인 시위를 시작했다. 이날 시위에는 오정훈 전국언론노동조합위원장, 전대식 지역신문노동조합협의회 의장(부산일보 지부장), 김명래 경인일보지부장, 민진영 경기민주언론시민연합 사무처장 등이 참여했다.\n",
    "\n",
    "이들은 1인 시위를 통해 ‘네이버 뉴스 서비스 지역 언론 배제 개선’을 요구하고 ‘지역 공론장 형성을 위한 네이버의 공적 책임’을 촉구했다. 전국언론노조와 민언련은 매주 월요일마다 네이버 본사 앞 시위를 진행할 예정이다.\n",
    "\n",
    "네이버는 최근 뉴스 서비스에 인공지능 에어스(AiRS) 알고리즘을 적용하는 방식으로 뉴스 편집 기능을 개편했다. 이후 지역 언론사 뉴스의 노출이 현격하게 줄어들었다. 아울러 100여 개 모바일 콘텐츠 제휴 언론사 중 독자가 구독을 선택할 수 있는 ‘채널’ 제휴사 44곳을 선정했는데 이 과정에서 지역 언론을 철저하게 배제했다. 이에 반발해 전국언론노조뿐 아니라 한국지방신문협회, 대한민국지방신문협의회 등 지역 신문 단체도 공동 성명을 발표하며 개선을 요구하고 나섰다. 전국시도의회의장협의회는 최근 총회에서 ‘네이버 지역 언론 배제 반대 성명’을 채택하기도 했다. 하지만 이 같은 반발에도 네이버는 뚜렷한 해결책을 내지 않고 있다.\n",
    "\n",
    "오정훈 위원장은 1인 시위에서 “네이버는 해결책을 내놓기는커녕 뉴스제휴평가위원회에만 책임을 떠넘기고 있다”며 “지역 언론 배제 문제의 해결을 위한 대화 요구에 즉각 응답해 뉴스 유통사업자로서의 사회적 역할을 수행해야 한다”고 촉구했다.\n",
    "\n",
    "전대식 의장은 “2005년 당시 네이버는 모든 언론사에 뉴스 유통망을 제공하는 우군이었지만 15년 만에 갑질하고 횡포하는 기업으로 전락했다”며 “지역 언론 종사자들이 만든 콘텐츠가 네이버에서 사라진 현실을 놓고 대화하고 상생적으로 풀어야 한다”면서 협의 창구 개설을 요구했다.\n",
    "\n",
    "민언련은 국내 1위 포털 네이버가 지역 공론장을 만드는 사회적 책임을 외면하는 것은 현 정부의 지방분권 강화 정책에 위배되는 것이라고 지적했다.\n",
    "\n",
    "민진영 사무처장은 “네이버가 지역신문에 대한 보도를 배제하고 시민들과의 소통 창구를 차단하는 것은 현 정부 정책인 지방분권 강화, 민주주의 정착이라는 큰 의제를 거역하는 것”이라며 “이런 행태는 반드시 중단돼야 하고 다양한 소식들이 소비자와 시민에게 전달돼야 한다”고 강조했다.\n",
    "\n",
    "출처 : 제주新보(http://www.jejunews.com)'''\n",
    "    \n",
    "result=re.findall(\"네이버\",text)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "text = '''전국 언론 노동자들이 시민사회단체와 함께 네이버의 지역 차별을 규탄하고 개선을 촉구하는 무기한 릴레이 시위에 돌입했다.\n",
    "\n",
    "전국언론노조와 전국 민주언론시민연합(민언련)은 1일 오전 11시 30분 경기도 성남시의 네이버 본사(그린팩토리) 앞에서 1인 시위를 시작했다. 이날 시위에는 오정훈 전국언론노동조합위원장, 전대식 지역신문노동조합협의회 의장(부산일보 지부장), 김명래 경인일보지부장, 민진영 경기민주언론시민연합 사무처장 등이 참여했다.\n",
    "\n",
    "이들은 1인 시위를 통해 ‘네이버 뉴스 서비스 지역 언론 배제 개선’을 요구하고 ‘지역 공론장 형성을 위한 네이버의 공적 책임’을 촉구했다. 전국언론노조와 민언련은 매주 월요일마다 네이버 본사 앞 시위를 진행할 예정이다.\n",
    "\n",
    "네이버는 최근 뉴스 서비스에 인공지능 에어스(AiRS) 알고리즘을 적용하는 방식으로 뉴스 편집 기능을 개편했다. 이후 지역 언론사 뉴스의 노출이 현격하게 줄어들었다. 아울러 100여 개 모바일 콘텐츠 제휴 언론사 중 독자가 구독을 선택할 수 있는 ‘채널’ 제휴사 44곳을 선정했는데 이 과정에서 지역 언론을 철저하게 배제했다. 이에 반발해 전국언론노조뿐 아니라 한국지방신문협회, 대한민국지방신문협의회 등 지역 신문 단체도 공동 성명을 발표하며 개선을 요구하고 나섰다. 전국시도의회의장협의회는 최근 총회에서 ‘네이버 지역 언론 배제 반대 성명’을 채택하기도 했다. 하지만 이 같은 반발에도 네이버는 뚜렷한 해결책을 내지 않고 있다.\n",
    "\n",
    "오정훈 위원장은 1인 시위에서 “네이버는 해결책을 내놓기는커녕 뉴스제휴평가위원회에만 책임을 떠넘기고 있다”며 “지역 언론 배제 문제의 해결을 위한 대화 요구에 즉각 응답해 뉴스 유통사업자로서의 사회적 역할을 수행해야 한다”고 촉구했다.\n",
    "\n",
    "전대식 의장은 “2005년 당시 네이버는 모든 언론사에 뉴스 유통망을 제공하는 우군이었지만 15년 만에 갑질하고 횡포하는 기업으로 전락했다”며 “지역 언론 종사자들이 만든 콘텐츠가 네이버에서 사라진 현실을 놓고 대화하고 상생적으로 풀어야 한다”면서 협의 창구 개설을 요구했다.\n",
    "\n",
    "민언련은 국내 1위 포털 네이버가 지역 공론장을 만드는 사회적 책임을 외면하는 것은 현 정부의 지방분권 강화 정책에 위배되는 것이라고 지적했다.\n",
    "\n",
    "민진영 사무처장은 “네이버가 지역신문에 대한 보도를 배제하고 시민들과의 소통 창구를 차단하는 것은 현 정부 정책인 지방분권 강화, 민주주의 정착이라는 큰 의제를 거역하는 것”이라며 “이런 행태는 반드시 중단돼야 하고 다양한 소식들이 소비자와 시민에게 전달돼야 한다”고 강조했다.\n",
    "\n",
    "출처 : 제주新보(http://www.jejunews.com)'''\n",
    "\n",
    "substring = \"네이버\"\n",
    "\n",
    "result=text.count(substring)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange', 'orange']\n"
     ]
    }
   ],
   "source": [
    "text = '''I like orange! I like orange!'''\n",
    "result = re.findall(\"orange\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like orange', 'like orange']\n"
     ]
    }
   ],
   "source": [
    "text = '''I like orange! I like orange!'''\n",
    "result = re.findall(\"like orange\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like']\n"
     ]
    }
   ],
   "source": [
    "text = '''I like orange! I like orange!'''\n",
    "result = re.findall(r\"^I like\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange!']\n"
     ]
    }
   ],
   "source": [
    "text = '''I like orange! I like orange!'''\n",
    "result = re.findall(\"orange!$\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$', '$']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall('\\$',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " '!',\n",
       " ' ',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '$',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " '!',\n",
       " ' ',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '$']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'.',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I li', 'ke o', 'rang', 'e! 2', '00$ ', 'I li', 'ke o', 'rang', 'e! 2']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'....',text)#.당 문자1개->4개씩 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'r', 'n', 'o', 'r', 'n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'[orn]',text)#or를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ora', 'ora']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'[orn][orn].',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '0', '0', '2', '0', '0']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'[0-9]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " 'I',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'[A-Za-z]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오', '렌', '지']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I like orange! 200$ 오렌지! 200$\"\n",
    "re.findall(r'[가-힣]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " '!',\n",
       " ' ',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '$',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '!',\n",
       " ' ',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '$']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'[^A-Za-z]',text)#[]안에서 ^는 not의 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange', 'orange']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'(orange)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'orange', 'like', 'orange']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'(orange|like)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! abc I like orange! ac'\n",
    "re.findall(r'a.c',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'ac']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! abc I like orange! ac'\n",
    "re.findall(r'a.?c',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'orange!', 'abc', 'I', 'like', 'orange!', 'ac', 'abbbc']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! abc I like orange! ac abbbc'\n",
    "re.findall(r'[^ ]+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'ac', 'abdbdc']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! abc I like orange! ac abdbdc'\n",
    "re.findall(r'a[bd]*c',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I lik', 'e ora', 'nge! ', '200$ ', 'I lik', 'e ora', 'nge! ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'.{5}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'abc', 'abb', 'abc', 'cab', 'cab']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'abcc abc abbab abccabcabd'\n",
    "re.findall(r'[abc]{3}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " 'I',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " '2',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'\\w',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '0', '0', '2', '0', '0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'\\d',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', ' ', ' ', ' ', ' ', ' ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I like orange! 200$ I like orange! 200$'\n",
    "re.findall(r'\\s',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010-2334-3234\n",
      "010-1321-4043\n",
      "016-444-3042\n"
     ]
    }
   ],
   "source": [
    "numbers = '''\n",
    "010-2334-3234\n",
    "02-302-3033\n",
    "010-1321-4043\n",
    "02-01-32\n",
    "33-3303-3033\n",
    "016-444-3042\n",
    "'''\n",
    "\n",
    "result = re.findall(r'[0-9]{3}-[0-9]{3,4}-[0-9]{4}',numbers)\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010-2334-3234\n",
      "010-1321-4043\n",
      "016-444-3042\n"
     ]
    }
   ],
   "source": [
    "numbers = '''\n",
    "010-2334-3234\n",
    "02-302-3033\n",
    "010-1321-4043\n",
    "02-01-32\n",
    "33-3303-3033\n",
    "016-444-3042\n",
    "'''\n",
    "\n",
    "result = re.finditer(r'[0-9]{3}-[0-9]{3,4}-[0-9]{4}',numbers)\n",
    "for i in result:\n",
    "    print(i.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앵커\n",
      "\n",
      "바로 좀 팩트체크를 해 보겠습니다 제모를 하면 마약검사에서 빠져나갈 수 있다 연예인 마약사건과 맞물려서 이런 글들이 온라인에서 확산됐습니다 수사기법을 비웃는 듯한 내용입니다 팩트체크팀이 국립과학수사연구원의 도움을 받아서 확인을 했습니다 결론은 마약 성분은 체모 외에도 온몸을 흔적을 남긴다는 겁니다\n",
      "오대영 기자 나와 있습니다 구체적으로 어떤 글들이 퍼져 있습니까\n",
      "\n",
      "기자\n",
      "\n",
      "전신 제모를 하면 문제가 없다 염색 탈색을 하면 된다 눈썹은 검사해도 소용없다 등의 내용입니다\n",
      "포털사이트에서 마약 검사라고 검색을 하면 모발 검사 안 걸리는 법이라는 연관 검색어까지 뜹니다\n"
     ]
    }
   ],
   "source": [
    "text = '''[앵커]\n",
    "\n",
    "바로 좀 팩트체크를 해 보겠습니다. 제모를 하면 마약검사에서 빠져나갈 수 있다. 연예인 마약사건과 맞물려서 이런 글들이 온라인에서 확산됐습니다. 수사기법을 비웃는 듯한 내용입니다. 팩트체크팀이 국립과학수사연구원의 도움을 받아서 확인을 했습니다. 결론은 마약 성분은 체모 외에도 온몸을 흔적을 남긴다는 겁니다.\n",
    "오대영 기자 나와 있습니다. 구체적으로 어떤 글들이 퍼져 있습니까?\n",
    "\n",
    "[기자]\n",
    "\n",
    "전신 제모를 하면 문제가 없다. 염색, 탈색을 하면 된다. 눈썹은 검사해도 소용없다 등의 내용입니다.\n",
    "포털사이트에서 마약 검사라고 검색을 하면 모발 검사 안 걸리는 법이라는 연관 검색어까지 뜹니다.'''\n",
    "\n",
    "results =re.sub(r'[\\[\\]?.,]',\"\",text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "jkilee@gmail.com\n",
    "kttredef@naver.com\n",
    "akdef!aa.com\n",
    "adekik@best.kr\n",
    "abkereff@aacde\n",
    "adefgree@korea.co.kr'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jkilee@gmail.com\n",
      "kttredef@naver.com\n",
      "adekik@best.kr\n",
      "adefgree@korea.co.kr\n"
     ]
    }
   ],
   "source": [
    "results = re.findall('[A-Za-z0-9]+@[a-z]+\\.[a-z].+',text)\n",
    "results = list(results)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jkilee@gmail.com\n",
      "kttredef@naver.com\n",
      "adekik@best.kr\n",
      "adefgree@korea.co.kr\n"
     ]
    }
   ],
   "source": [
    "emails = '''\n",
    "jkilee@gmail.com\n",
    "kttredef@naver.com\n",
    "akdef!aa.com\n",
    "adekik@best.kr\n",
    "abkereff@aacde\n",
    "adefgree@korea.co.kr\n",
    "'''\n",
    "\n",
    "for email in emails.split('\\n'):\n",
    "    matched = re.search('[a-zA-Z].*@[a-zA-Z].+\\.[a-zA-Z].+',email)\n",
    "    if matched:\n",
    "        print(matched.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 저는 홍길동 입니다 나이는 24살 세계 최고의  데이터 분석가가 되고싶습니다\n"
     ]
    }
   ],
   "source": [
    "text = '''안녕하세요 저는 <em>홍길동</em> 입니다. 나이는 24살 세계 최고의 <a href=\"aa.aa.com\">데이터 분석가</a>가 되고싶습니다.'''\n",
    "\n",
    "results = re.sub(r'[\\</=.\"\\>A-Za-z]',\"\",text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 저는 홍길동 입니다. 나이는 24살 세계 최고의 데이터 분석가가 되고싶습니다.\n"
     ]
    }
   ],
   "source": [
    "text = '''안녕하세요 저는 <em>홍길동</em> 입니다. 나이는 24살 세계 최고의 <a href=\"aa.aa.com\">데이터 분석가</a>가 되고싶습니다.'''\n",
    "\n",
    "results = re.sub(r'<[^<]*>',\"\",text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 저는 홍길동 입니다. 나이는 24살 세계 최고의 데이터 분석가가 되고싶습니다.\n"
     ]
    }
   ],
   "source": [
    "text = '''안녕하세요 저는 <em>홍길동</em> 입니다. 나이는 24살 세계 최고의 <a href=\"aa.aa.com\">데이터 분석가</a>가 되고싶습니다.'''\n",
    "\n",
    "results = re.sub(r'<.+?>',\"\",text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['네이버가 뉴스 서비스에 인공지능(AI)을 도입해 페이지 뷰(PV)를 늘리고 이용자를 끌어 모으고 있다.',\n",
       " '네이버는 5일 오전 서울 강남구 그랜드 인터컨티넨털 호텔에서 AI 콜로키움 2019를 열고 이 같은 AI 성과와 전략을 소개했다.',\n",
       " '이날 기조연설에서 김광현 네이버 서치앤클로바 리더는 \"AI 뉴스 추천 시스템인 에어스(AiRS)를 도입하면서 뉴스 소비량이 확대되고 있다\" 고 말했다.']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''<p>\n",
    "<span>네이버가 뉴스 서비스에 인공지능(AI)을 도입해 페이지 뷰(PV)를 늘리고 이용자를 끌어 모으고 있다.</span>\n",
    "<span>네이버는 5일 오전 서울 강남구 그랜드 인터컨티넨털 호텔에서 AI 콜로키움 2019를 열고 이 같은 AI 성과와 전략을 소개했다.</span>\n",
    "<span>이날 기조연설에서 김광현 네이버 서치앤클로바 리더는 \"AI 뉴스 추천 시스템인 에어스(AiRS)를 도입하면서 뉴스 소비량이 확대되고 있다\" 고 말했다.</span>\n",
    "</p>'''\n",
    "\n",
    "results = re.sub(r'<[^<]*>',\"\",text)\n",
    "results = results.strip(\"\\n\")\n",
    "result = re.split(r'\\n',results)\n",
    "result = list(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '네이버가 뉴스 서비스에 인공지능(AI)을 도입해 페이지 뷰(PV)를 늘리고 이용자를 끌어 모으고 있다.', '네이버는 5일 오전 서울 강남구 그랜드 인터컨티넨털 호텔에서 AI 콜로키움 2019를 열고 이 같은 AI 성과와 전략을 소개했다.', '이날 기조연설에서 김광현 네이버 서치앤클로바 리더는 \"AI 뉴스 추천 시스템인 에어스(AiRS)를 도입하면서 뉴스 소비량이 확대되고 있다\" 고 말했다.', '']\n"
     ]
    }
   ],
   "source": [
    "text = '''<p>\n",
    "<span>네이버가 뉴스 서비스에 인공지능(AI)을 도입해 페이지 뷰(PV)를 늘리고 이용자를 끌어 모으고 있다.</span>\n",
    "<span>네이버는 5일 오전 서울 강남구 그랜드 인터컨티넨털 호텔에서 AI 콜로키움 2019를 열고 이 같은 AI 성과와 전략을 소개했다.</span>\n",
    "<span>이날 기조연설에서 김광현 네이버 서치앤클로바 리더는 \"AI 뉴스 추천 시스템인 에어스(AiRS)를 도입하면서 뉴스 소비량이 확대되고 있다\" 고 말했다.</span>\n",
    "</p>'''\n",
    "\n",
    "results = re.sub(r'<[^<]*>',\"\",text)\n",
    "results = results.split('\\n')\n",
    "results = list(results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['삼성전자', '55,200'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['KT&G', '92,500'], ['삼성전자우', '45,600'], ['대양금속', '10,550'], ['SK하이닉스', '94,700'], ['SK텔레콤', '234,000']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20'], ['니케이225', '23,656.62']]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result_pop = []\n",
    "\n",
    "li_list = soup.select('ul[class=\"lst_pop\"] > li')\n",
    "for li in li_list:\n",
    "    result_pop.append([li.select_one('a').text,li.select_one('span').text])\n",
    "print(result_pop)\n",
    "\n",
    "result_major = []\n",
    "\n",
    "li_major = soup.select('ul[class=\"lst_major\"] > li')\n",
    "for a in li_major:\n",
    "    result_major.append([a.select_one('a').text,a.select_one('span').text])\n",
    "print(result_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '상한'], ['삼성전자', '하락'], ['안랩', '상승'], ['케이엠더블..', '상승'], ['피피아이', '상한'], ['KT&G', '하락'], ['삼성전자우', '상승'], ['대양금속', '하한'], ['SK하이닉스', '상승'], ['SK텔레콤', '하락']]\n",
      "[['다우산업', '상한'], ['나스닥', '상한'], ['홍콩H', '상한'], ['상해종합', '상한'], ['니케이225', '하락']]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result_pop = []\n",
    "\n",
    "li_list = soup.select('ul[class=\"lst_pop\"] > li')\n",
    "for li in li_list:\n",
    "    result_pop.append([li.select_one('a').text,li.select_one('img').attrs['alt']])\n",
    "print(result_pop)\n",
    "\n",
    "result_major = []\n",
    "\n",
    "li_major = soup.select('ul[class=\"lst_major\"] > li')\n",
    "for a in li_major:\n",
    "    result_major.append([a.select_one('a').text,a.select_one('img').attrs['alt']])\n",
    "print(result_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['삼성전자우', '45,600'], ['SK하이닉스', '94,700']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20']]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result_pop = []\n",
    "\n",
    "li_list = soup.select('ul[class=\"lst_pop\"] > li[class=\"up\"]')\n",
    "for li in li_list:\n",
    "    result_pop.append([li.select_one('a').text,li.select_one('span').text])\n",
    "print(result_pop)\n",
    "\n",
    "result_major = []\n",
    "\n",
    "li_major = soup.select('ul[class=\"lst_major\"] > li[class=\"up\"]')\n",
    "for a in li_major:\n",
    "    result_major.append([a.select_one('a').text,a.select_one('span').text])\n",
    "print(result_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['삼성전자우', '45,600'], ['SK하이닉스', '94,700']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20']]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result_pop = []\n",
    "\n",
    "li_list = soup.select('ul[class=\"lst_pop\"] > li')\n",
    "for li in li_list:\n",
    "    if li.attrs['class'][0] == 'up':\n",
    "        result_pop.append([li.select_one('a').text,li.select_one('span').text])\n",
    "print(result_pop)\n",
    "\n",
    "result_major = []\n",
    "\n",
    "li_major = soup.select('ul[class=\"lst_major\"] > li')\n",
    "for li in li_major:\n",
    "    if li.attrs['class'][0] == 'up':\n",
    "        result_major.append([li.select_one('a').text,li.select_one('span').text])\n",
    "print(result_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'이름': 'H하우스장위',\n",
       "  '분양가': '16,000 ',\n",
       "  '유형': '아파트',\n",
       "  '분양유형': '일반민간임대',\n",
       "  '세대수': '분양 134세대',\n",
       "  '평형': '45㎡~65㎡'},\n",
       " {'이름': '고덕리엔파크2단지 장기전세',\n",
       "  '분양가': '38,400 ',\n",
       "  '유형': '아파트',\n",
       "  '분양유형': '장기전세주택',\n",
       "  '세대수': '분양 1세대',\n",
       "  '평형': '149㎡'},\n",
       " {'이름': '신정이펜하우스3단지 장기전세',\n",
       "  '분양가': '39,040 ',\n",
       "  '유형': '아파트',\n",
       "  '분양유형': '장기전세주택',\n",
       "  '세대수': '분양 1세대',\n",
       "  '평형': '148㎡'},\n",
       " {'이름': '천왕이펜하우스2단지 장기전세',\n",
       "  '분양가': '38,240 ',\n",
       "  '유형': '아파트',\n",
       "  '분양유형': '장기전세주택',\n",
       "  '세대수': '분양 1세대',\n",
       "  '평형': '142㎡'},\n",
       " {'이름': '송파파크데일2단지 장기전세',\n",
       "  '분양가': '45,600 ',\n",
       "  '유형': '아파트',\n",
       "  '분양유형': '장기전세주택',\n",
       "  '세대수': '분양 1세대',\n",
       "  '평형': '150㎡'}]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "index = ['이름','분양가','유형','분양유형','세대수','평형']\n",
    "\n",
    "name = soup.select('ul[class=\"sale_list _sale_list\"] a[target=\"_blank\"]')\n",
    "name_list = []\n",
    "for i in range(len(name)):\n",
    "    name_list.append(name[i].text)\n",
    "#print(name_list)\n",
    "\n",
    "price = soup.select('dl[class=\"detail_info\"] dd')\n",
    "price_list = []\n",
    "for i in range(len(price)):\n",
    "    price_list.append(price[i].text)\n",
    "#print(price_list)\n",
    "\n",
    "value = []\n",
    "for i in range(len(name)):\n",
    "    value.append([name_list[i],price_list[4*i][:-2],price_list[4*i+1][:3],\n",
    "                  price_list[4*i+1][4:],price_list[4*i+2].split(\"|\")[0],\n",
    "                  price_list[4*i+2].split(\"|\")[1],price_list[4*i+3]])\n",
    "#print(value)\n",
    "\n",
    "result = []\n",
    "for i in range(len(name)):\n",
    "    dic = dict(zip(index,value[i]))\n",
    "    result.append(dic)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dd class=\"txt\"><strong class=\"point\">45,600</strong> 만원</dd>,\n",
       " <dd class=\"txt\">아파트<span class=\"bar\">|</span>장기전세주택</dd>,\n",
       " <dd class=\"txt\">분양 1세대<span class=\"bar\">|</span>150㎡</dd>,\n",
       " <dd class=\"txt\">서울시 송파구 마천동</dd>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "total = []\n",
    "index = ['이름','분양가','유형','분양유형','세대수','평형']\n",
    "\n",
    "li_list = soup.select(\"ul.sale_list > li\")\n",
    "\n",
    "for li in li_list:\n",
    "    result = {}\n",
    "    result['이름'] = li.select_one(\"a\").text \n",
    "    \n",
    "    dds = li.select_one(\"dl.detail_info\").select(\"dd\")\n",
    "    result['분양가'] = ''.join(re.findall(r'\\d',dds[0].text))\n",
    "    \n",
    "    types = dds[1].text.split('|')\n",
    "    result['유형'] = types[0]\n",
    "    result['분양유형'] = types[1]\n",
    "    \n",
    "    sizes = dds[2].text.split('|')\n",
    "    result['세대수'] = sizes[0]\n",
    "    result['평형'] = sizes[1]\n",
    "    \n",
    "    total.append(result)\n",
    "    \n",
    "# total\n",
    "dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 조제\n",
      "2 원더 우먼 1984\n",
      "3 이웃사촌\n",
      "4 도굴\n",
      "5 언플랜드\n",
      "6 런\n",
      "7 러브 액츄얼리\n",
      "8 극장판 바이올렛 에버가든\n",
      "9 800\n",
      "10 호프\n",
      "11 리플레이\n",
      "12 테넷\n",
      "13 개 같은 것들\n",
      "14 퍼스트 러브\n",
      "15 콜\n",
      "16 삼진그룹 영어토익반\n",
      "17 미드나이트 스카이\n",
      "18 발레리안: 천 개 행성의 도시\n",
      "19 더 프롬\n",
      "20 잔칫날\n",
      "21 블랙 위도우\n",
      "22 스웨그\n",
      "23 덩케르크\n",
      "24 라이어트: 기계들의 역습\n",
      "25 러브레터\n",
      "26 나이팅게일\n",
      "27 파티마의 기적\n",
      "28 뱅가드\n",
      "29 인터스텔라\n",
      "30 조제, 호랑이 그리고 물고기들\n",
      "31 애비규환\n",
      "32 모텔리어\n",
      "33 세트플레이\n",
      "34 가나의 혼인잔치: 언약\n",
      "35 집착\n",
      "36 그날이 온다\n",
      "37 그린 북\n",
      "38 썸머 85\n",
      "39 내 어깨 위 고양이, 밥 2\n",
      "40 아카이브\n",
      "41 뮤직 앤 리얼리티\n",
      "42 국제수사\n",
      "43 누군가 어디에서 나를 기다리면 좋겠다\n",
      "44 소년시절의 너\n",
      "45 화양연화\n",
      "46 먼 훗날 우리\n",
      "47 007 노 타임 투 다이\n",
      "48 운디네\n",
      "49 그린랜드\n",
      "50 라라랜드\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://movie.naver.com/movie/sdb/rank/rmovie.nhn'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = soup.select('.list_ranking tr > td.title a')\n",
    "\n",
    "for i,v in enumerate(result):\n",
    "    print(i+1,v.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.naver.com/sise/sise_quant.nhn'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "name = soup.select('.box_type_l > .type_2  a')\n",
    "# price = soup.select('.box_type_l > .type_2 td.number')\n",
    "t_price = soup.find_all('td',class_='number')\n",
    "t_margin = soup.find_all('span',class_='tah')\n",
    "\n",
    "# for i,v in enumerate(t_margin):\n",
    "#     if i % 2 == 0:\n",
    "#         margin = v.text    \n",
    "#         print(margin)\n",
    "        \n",
    "        \n",
    "# for idx,val in enumerate(name):\n",
    "#      print(idx+1,val.text)\n",
    "        \n",
    "# t_price = list(t_price)\n",
    "\n",
    "# for i in range(len(t_price)):\n",
    "#     print(t_price[i*10].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 KODEX 200선물인버스2X 2,870\n",
      "2 KODEX 코스닥150선물인버스 4,605\n",
      "3 현대약품 8,120\n",
      "4 금호에이치티 2,530\n",
      "5 KODEX 레버리지 20,775\n",
      "6 KODEX 코스닥150 레버리지 15,505\n",
      "7 LG디스플레이 18,250\n",
      "8 미래아이앤지 384\n",
      "9 KODEX 인버스 4,545\n",
      "10 서울식품 249\n",
      "11 알루코 5,170\n",
      "12 영진약품 8,280\n",
      "13 까뮤이앤씨 3,000\n",
      "14 계양전기 5,070\n",
      "15 이아이디 361\n",
      "16 삼성전자 73,900\n",
      "17 TCC스틸 5,640\n",
      "18 KEC 2,935\n",
      "19 대한전선 1,350\n",
      "20 LG전자 119,500\n",
      "21 명문제약 7,970\n",
      "22 신성이엔지 3,370\n",
      "23 KODEX 코스닥 150 14,685\n",
      "24 국제약품 10,450\n",
      "25 남성 4,210\n",
      "26 삼성중공업 6,960\n",
      "27 팬오션 4,530\n",
      "28 디아이씨 2,955\n",
      "29 삼성 레버리지 WTI원유 선물 ETN 400\n",
      "30 ESR켄달스퀘어리츠 5,190\n",
      "31 세화아이엠씨 543\n",
      "32 현대퓨처넷 3,800\n",
      "33 삼부토건 4,430\n",
      "34 쎌마테라퓨틱스 6,580\n",
      "35 KODEX 200 37,110\n",
      "36 신한 레버리지 WTI원유 선물 ETN(H) 345\n",
      "37 유나이티드제약 64,800\n",
      "38 아남전자 2,320\n",
      "39 한국전력 26,800\n",
      "40 써니전자 4,630\n",
      "41 명신산업 45,700\n",
      "42 대영포장 1,715\n",
      "43 LG 86,400\n",
      "44 대웅 54,400\n",
      "45 한온시스템 16,150\n",
      "46 영화금속 2,190\n",
      "47 LG유플러스 12,050\n",
      "48 한화생명 2,440\n",
      "49 삼아알미늄 11,400\n",
      "50 유니퀘스트 14,200\n",
      "51 대한해운 2,935\n",
      "52 덕양산업 1,740\n",
      "53 화천기계 2,695\n",
      "54 TIGER 200선물인버스2X 2,970\n",
      "55 만도 59,000\n",
      "56 부광약품 29,000\n",
      "57 SK증권 792\n",
      "58 HMM 13,050\n",
      "59 후성 10,950\n",
      "60 에이프로젠제약 1,300\n",
      "61 태림포장 6,250\n",
      "62 아시아나항공 4,210\n",
      "63 LG상사 25,100\n",
      "64 신풍제지 4,160\n",
      "65 조일알미늄 780\n",
      "66 한창 1,410\n",
      "67 LG이노텍 185,000\n",
      "68 DB하이텍 40,100\n",
      "69 삼성제약 4,430\n",
      "70 SK하이닉스 116,000\n",
      "71 KR모터스 1,115\n",
      "72 삼성 인버스 2X WTI원유 선물 ETN 1,405\n",
      "73 진원생명과학 20,500\n",
      "74 LG전자우 47,500\n",
      "75 파미셀 16,750\n",
      "76 에이프로젠 KIC 2,000\n",
      "77 신풍제약 113,500\n",
      "78 오리엔트바이오 1,230\n",
      "79 두산중공업 13,100\n",
      "80 보해양조 923\n",
      "81 대우건설 4,480\n",
      "82 에넥스 1,195\n",
      "83 필룩스 3,930\n",
      "84 동화약품 20,450\n",
      "85 우리종금 541\n",
      "86 삼성전자우 69,900\n",
      "87 콤텍시스템 1,515\n",
      "88 TIGER 200 IT 32,890\n",
      "89 한화솔루션 46,400\n",
      "90 KODEX 2차전지산업 14,845\n",
      "91 한창제지 2,505\n",
      "92 TIGER 200선물레버리지 15,585\n",
      "93 두산인프라코어 8,080\n",
      "94 세방전지 61,200\n",
      "95 광동제약 10,300\n",
      "96 보락 2,505\n",
      "97 한솔홈데코 2,030\n",
      "98 큐로 625\n",
      "99 남선알미늄 4,610\n",
      "100 쌍방울 623\n"
     ]
    }
   ],
   "source": [
    "url = 'https://finance.naver.com/sise/sise_quant.nhn'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = []\n",
    "tr_list = soup.select('div.box_type_l > table.type_2')\n",
    "\n",
    "for tr in tr_list:\n",
    "    name = tr.select('a') #종목명\n",
    "    price = tr.select('td.number') #현재가\n",
    "    for i in range(len(name)):\n",
    "        print(i+1,name[i].text,price[i*10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.naver.com/sise/sise_quant.nhn'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "tr_list = soup.select('div.box_type_l > table.type_2')\n",
    "\n",
    "for tr in tr_list:\n",
    "    if tr.select_one('img').attrs['alt'] == '상승':\n",
    "        name = tr.select('a')\n",
    "        price = tr.select('td.number')\n",
    "        \n",
    "        for i in range(len(name)):\n",
    "            print(i+1,name[i].text,price[i*10].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         name = tr.select('a')\n",
    "#         price = tr.select('td.number')\n",
    "\n",
    "\n",
    "#         for i in range(len(name)):\n",
    "#             print(i+1,name[i].text,price[i*10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상한\n",
      "하락\n",
      "상승\n",
      "상승\n",
      "상한\n",
      "하락\n",
      "상승\n",
      "하한\n",
      "상승\n",
      "하락\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/03/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = []\n",
    "li_list = soup.select(\"#popularItemList > li\")\n",
    "for li in li_list:\n",
    "    print(li.select_one('img').attrs['alt'])\n",
    "#     result.append([li.select_one('a').text, li.select_one('span').text])\n",
    "    \n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study2",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
