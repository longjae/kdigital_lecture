{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    전통적인 요리법이나 양식은 상당한 차이가 있지만, 이탈리아 요리는 다른 국가의 요리 문화에서 다양한 영감을 줄 만큼 다양하고 혁신적인 것으로 평가되고 있다. 각 지방마다 고유의 특색이 있어 그 양식도 다양하지만 크게 북부와 남부로 나눌 수 있다. 다른 나라와 국경을 맞대고 있던 북부 지방은 산업화되어 경제적으로 풍족하고 농업이 발달해 쌀이 풍부해 유제품이 다양한 반면 경제적으로 침체되었던 남부 지방은 올리브와 토마토, 모차렐라 치즈가 유명하고 특별히 해산물을 활용한 요리가 많다. 식재료와 치즈 등의 차이는 파스타의 종류와 소스와 수프 등도 다름을 의미한다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL='https://scrapying-study.firebaseapp.com/01/'\n",
    "\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "results=soup.find(id=\"cook\")\n",
    "\n",
    "print(results.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'이름': '이몽룡', '나이': '34'}, {'이름': '홍길동', '나이': '23'}]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/01/'\n",
    "\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "a = soup.find_all(\"th\")\n",
    "b = soup.find_all(\"td\")\n",
    "\n",
    "keys=[]\n",
    "for i in range(len(a)):\n",
    "    keys.append(a[i].text)\n",
    "\n",
    "values=[]\n",
    "for i in range(len(b)):\n",
    "    values.append(b[i].text)\n",
    "\n",
    "result=[]\n",
    "for i in range(int(len(b)/len(a))):\n",
    "    dic = {keys[j]:values[i*2+j] for j in range(len(a))}\n",
    "    result.append(dic)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'이름': '이몽룡', '나이': '34'}, {'이름': '홍길동', '나이': '23'}]\n"
     ]
    }
   ],
   "source": [
    "response=requests.get('https://scrapying-study.firebaseapp.com/01/')\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = []\n",
    "\n",
    "headers = soup.find_all(\"th\")\n",
    "keys = []\n",
    "for th in headers:\n",
    "    keys.append(th.text)\n",
    "    \n",
    "tbody = soup.find(\"tbody\")\n",
    "tr_list = tbody.find_all(\"tr\")\n",
    "for tr in tr_list:\n",
    "    td_list = tr.find_all(\"td\")\n",
    "    values = []\n",
    "    for td in td_list:\n",
    "        values.append(td.text)\n",
    "        \n",
    "    result.append(dict(zip(keys,values)))\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    크롤링 연습사이트 01-1 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-2 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-3 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-4 페이지입니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'04.html'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = soup.find_all(\"a\")\n",
    "\n",
    "for page in pages :\n",
    "    page_link = (\"https://scrapying-study.firebaseapp.com/01/\" + page[\"href\"])\n",
    "    response_new = requests.get(page_link)\n",
    "    soup_new = BeautifulSoup(response_new.text, \"html.parser\")\n",
    "    result_new = soup_new.find(\"p\")\n",
    "    \n",
    "    print(result_new.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    크롤링 연습사이트 01-1 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-2 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-3 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-4 페이지입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url='https://scrapying-study.firebaseapp.com/01/'\n",
    "\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "a_list = soup.find_all(\"a\")\n",
    "\n",
    "for a in a_list:\n",
    "    new_url = url+a.attrs['href']\n",
    "    res = requests.get(new_url)\n",
    "    s = BeautifulSoup(res.text,\"html.parser\")\n",
    "    print(s.find(\"p\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<title>크롤링 연습사이트 02</title>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select(\"title\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"bold blue\" id=\"title\">\n",
      "        안녕하세요 \n",
      "    </div>\n",
      "\n",
      "\n",
      "<div class=\"bold\" id=\"content\">\n",
      "<ul>\n",
      "<li>첫번쨰리스트</li>\n",
      "<li class=\"blue\">두번째리스트</li>\n",
      "<li class=\"blue\">세번째리스트</li>\n",
      "<li>네번째리스트</li>\n",
      "</ul>\n",
      "</div>\n",
      "\n",
      "\n",
      "<div id=\"winter\">\n",
      "<p>온세상이 떨릴듯</p>\n",
      "<p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>\n",
      "<div>\n",
      "<p>네가 느껴져</p>\n",
      "<p class=\"blue\">오래된 친구같아</p>\n",
      "<span>안녕하세요!</span>\n",
      "</div>\n",
      "</div>\n",
      "\n",
      "\n",
      "<div>\n",
      "<p>네가 느껴져</p>\n",
      "<p class=\"blue\">오래된 친구같아</p>\n",
      "<span>안녕하세요!</span>\n",
      "</div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select(\"div\")\n",
    "for div in result:\n",
    "    print(div)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        안녕하세요 \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select(\"#title\")\n",
    "\n",
    "print(result[0].text)#객체 가져오는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"bold blue\" id=\"title\">\n",
      "        안녕하세요 \n",
      "    </div>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select(\"#title\")\n",
    "\n",
    "print(result)#객체 가져오는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"bold blue\" id=\"title\">\n",
      "        안녕하세요 \n",
      "    </div>, <div class=\"bold\" id=\"content\">\n",
      "<ul>\n",
      "<li>첫번쨰리스트</li>\n",
      "<li class=\"blue\">두번째리스트</li>\n",
      "<li class=\"blue\">세번째리스트</li>\n",
      "<li>네번째리스트</li>\n",
      "</ul>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select(\".bold\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://naver.com\" target=\"_blank\">네이버</a>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select('a[target=\"_blank\"]') #result = soup.select('a[herf=\"http://naver.com\"]')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://naver.com\" target=\"_blank\">네이버</a>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select('a[href*=\"naver\"]')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>온세상이 떨릴듯</p>, <p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>, <p>네가 느껴져</p>, <p class=\"blue\">오래된 친구같아</p>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select('div#winter p')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>온세상이 떨릴듯</p>, <p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select('div#winter > p')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>, <p class=\"blue\">오래된 친구같아</p>]\n"
     ]
    }
   ],
   "source": [
    "URL='https://scrapying-study.firebaseapp.com/02/'\n",
    "response=requests.get(URL)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select('p.blue')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'써니전자': '5,000', '삼성전자': '55,200', '안랩': '81,000', '케이엠더블..': '57,300', '피피아이': '12,600', 'KT&G': '92,500', '삼성전자우': '45,600', '대양금속': '10,550', 'SK하이닉스': '94,700', 'SK텔레콤': '234,000'}]\n",
      "[{'다우산업': '28,647.43', '나스닥': '9,015.03', '홍콩H': '11,320.56', '상해종합': '3,085.20', '니케이225': '23,656.62'}]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "response=requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = []\n",
    "result_foreign = []\n",
    "\n",
    "result_pop = soup.select('.lst_pop a[href]')\n",
    "pop_list = []\n",
    "for idx,val in enumerate(result_pop):\n",
    "    pop_list.append(result_pop[idx].text)\n",
    "\n",
    "result_price = soup.select('.lst_pop span')\n",
    "price_list = []\n",
    "for idx,val in enumerate(result_price):\n",
    "    price_list.append(result_price[idx].text)\n",
    "\n",
    "result.append(dict(zip(pop_list,price_list))) #인기검색종목\n",
    "print(result)\n",
    "\n",
    "result_major = soup.select('.lst_major a[href]')\n",
    "major_list = []\n",
    "for idx,val in enumerate(result_major):\n",
    "    major_list.append(result_major[idx].text)\n",
    "    \n",
    "result_mprice = soup.select('.lst_major span')\n",
    "major_price = []\n",
    "for idx,val in enumerate(result_mprice):\n",
    "    major_price.append(result_mprice[idx].text)\n",
    "    \n",
    "result_foreign.append(dict(zip(major_list,major_price))) #주요해외지수\n",
    "print(result_foreign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['삼성전자', '55,200'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['KT&G', '92,500'], ['삼성전자우', '45,600'], ['대양금속', '10,550'], ['SK하이닉스', '94,700'], ['SK텔레콤', '234,000']]\n",
      "[['다우산업', '28,647.43'], ['니케이225', '9,015.03'], ['상해종합', '11,320.56'], ['홍콩H', '3,085.20'], ['나스닥', '23,656.62']]\n"
     ]
    }
   ],
   "source": [
    "#실습 01\n",
    "\n",
    "url = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "response=requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "\n",
    "result_pop = soup.select('.lst_pop a[href]')\n",
    "pop_list = []\n",
    "for idx,val in enumerate(result_pop):\n",
    "    pop_list.append(result_pop[idx].text)\n",
    "#print(pop_list)\n",
    "\n",
    "result_major = soup.select('.lst_major a[href]')\n",
    "major_list = []\n",
    "for idx,val in enumerate(result_major):\n",
    "    major_list.append(result_major[idx].text)\n",
    "#print(major_list)\n",
    "\n",
    "span = soup.select('li > span')\n",
    "span_list = []\n",
    "for idx,val in enumerate(span):\n",
    "    span_list.append(span[idx].text)\n",
    "#print(span_list)\n",
    "\n",
    "list_pop = []\n",
    "list_major = []\n",
    "\n",
    "for i in range(len(span)):\n",
    "    if i < len(result_pop):\n",
    "        list_pop.append([pop_list[i],span_list[i]])\n",
    "    else:\n",
    "        list_major.append([major_list[len(result_pop)-i],span_list[i]])\n",
    "        \n",
    "print(list_pop)\n",
    "print(list_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['삼성전자', '55,200'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['KT&G', '92,500'], ['삼성전자우', '45,600'], ['대양금속', '10,550'], ['SK하이닉스', '94,700'], ['SK텔레콤', '234,000']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20'], ['니케이225', '23,656.62']]\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "a = soup.select('a[href*=\"item\"]')\n",
    "index = []\n",
    "for i in range(len(a)):\n",
    "    index.append(a[i].text)\n",
    "# print(index)\n",
    "\n",
    "b = soup.select('a[href*=\"world\"]')\n",
    "index2 = []\n",
    "for i in range(len(b)):\n",
    "    index2.append(b[i].text)\n",
    "# print(index2)\n",
    "\n",
    "c = soup.select('li>span')\n",
    "value = []\n",
    "for i in range(len(c)):\n",
    "    value.append(c[i].text)\n",
    "# print(value)\n",
    "\n",
    "result = []\n",
    "result2 = []\n",
    "\n",
    "for i in range(len(c)):\n",
    "    if i < len(a):\n",
    "        result.append([index[i],value[i]])\n",
    "    else:\n",
    "        result2.append([index2[i-len(a)],value[i]])\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<img alt=\"상한\" height=\"11\" src=\"https://ssl.pstatic.net/static/nfinance/ico_up2.gif\" width=\"7\"/>, <img alt=\"하락\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_down.gif\" width=\"7\"/>, <img alt=\"상승\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_up.gif\" width=\"7\"/>, <img alt=\"상승\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_up.gif\" width=\"7\"/>, <img alt=\"상한\" height=\"11\" src=\"https://ssl.pstatic.net/static/nfinance/ico_up2.gif\" width=\"7\"/>, <img alt=\"하락\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_down.gif\" width=\"7\"/>, <img alt=\"상승\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_up.gif\" width=\"7\"/>, <img alt=\"하한\" height=\"11\" src=\"https://ssl.pstatic.net/static/nfinance/ico_down2.gif\" width=\"7\"/>, <img alt=\"상승\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_up.gif\" width=\"7\"/>, <img alt=\"하락\" height=\"6\" src=\"https://ssl.pstatic.net/static/nfinance/ico_down.gif\" width=\"7\"/>]\n"
     ]
    }
   ],
   "source": [
    "#실습 02\n",
    "\n",
    "URL = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "img = soup.select('ul[class=\"lst_pop\"] > li[class] > img')\n",
    "img_list = []\n",
    "\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['삼성전자우', '45,600'], ['SK하이닉스', '94,700']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20']]\n"
     ]
    }
   ],
   "source": [
    "#실습03\n",
    "# 국내주식\n",
    "a = soup.select('ul[class=\"lst_pop\"] > li[class=\"up\"] > a')\n",
    "index = []\n",
    "for i in range(len(a)):\n",
    "    index.append(a[i].text)\n",
    "b = soup.select('ul[class=\"lst_pop\"] > li[class=\"up\"] > span')\n",
    "value = []\n",
    "for i in range(len(b)):\n",
    "    value.append(b[i].text)\n",
    "result = []\n",
    "for i in range(len(a)):\n",
    "        result.append([index[i],value[i]])\n",
    "print(result)\n",
    "\n",
    "# 해외주식\n",
    "c = soup.select('ul[class=\"lst_major\"] > li[class=\"up\"] > a')\n",
    "index = []\n",
    "for i in range(len(c)):\n",
    "    index.append(c[i].text)\n",
    "d = soup.select('ul[class=\"lst_major\"] > li[class=\"up\"] > span')\n",
    "value = []\n",
    "for i in range(len(d)):\n",
    "    value.append(d[i].text)\n",
    "result2 = []\n",
    "for i in range(len(c)):\n",
    "        result2.append([index[i],value[i]])\n",
    "print(result2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>, <span class=\"bar\">|</span>]\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://scrapying-study.firebaseapp.com/03/'\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "name = soup.select('a[target=\"_blank\"]')\n",
    "name_list = []\n",
    "for idx,val in enumerate(name):\n",
    "    name_list.append(name[idx].text)\n",
    "#print(name_list)\n",
    "\n",
    "price = soup.select('strong[class=\"point\"]')\n",
    "price_list = []\n",
    "for idx,val in enumerate(price):\n",
    "    price_list.append(price[idx].text)\n",
    "#print(price_list)\n",
    "\n",
    "class1 = soup.select('dd[class~=\"아파트\"]') #미해결\n",
    "class1_list = []\n",
    "for idx,val in enumerate(class1):\n",
    "    class1_list.append(class1[idx].text)\n",
    "#print(class1_list)\n",
    "\n",
    "class2 = soup.select('span[class=\"bar\"]')\n",
    "class2_list = []\n",
    "for idx,val in enumerate(class2):\n",
    "    class2_list.append(class2[idx].text)\n",
    "print(class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "text = \"I like orange! I love orange!\"\n",
    "result = re.match(\"orange\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study2",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
